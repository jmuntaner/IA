\documentclass[12pt]{report}

\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor = blue
}
\usepackage{amsmath,amsfonts,mathtools,cancel,amssymb,physics,float}
\usepackage{tikz,subcaption,pgf,pgfplots,listings,color,framed}
\usepackage[autostyle]{csquotes}
\usetikzlibrary{external,arrows}
\tikzexternalize[prefix=figures/]
\pgfplotsset{compat=1.11}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle,basicstyle=\ttfamily}

\newcommand*\dif{\mathop{}\!\mathrm{d}}

%\usepackage[margin=1in,a4paper]{geometry}
\usepackage{parskip}

% Harvard bibliography style for referencing
\usepackage[style=alphabetic]{biblatex}

% Bibliography in toc
\usepackage[nottoc,numbib]{tocbibind}

% Add bibliography file
\addbibresource{ref.bib}

\begin{document}

\begin{titlepage}
    \centering
    \vspace{0.5 cm}
    \begin{center}
        \textsc{\Large Universitat Politècnica de Catalunya}\\[2.0 cm]
    \end{center}% University Name
    \textsc{\Large Trabajo de investigación }\\[0.5 cm]				% Course Code
    \rule{\linewidth}{0.2 mm} \\[0.4 cm]
    { \huge \bfseries Neural Ordinary Differential Equations}\\
    \rule{\linewidth}{0.2 mm} \\[1.5 cm]
    
    \vfill

    \begin{minipage}{\textwidth}

        \begin{flushright} \large
            \emph{Realizado por :} \\
            Lanchares Sánchez, Ernesto \\
            López Guitart, Ferran
        \end{flushright}

    \end{minipage}\\[2 cm]

    \includegraphics[scale = 0.3]{UPCLogo.png}
    \vspace{0.5cm}
\end{titlepage}


\tableofcontents

\chapter{Introducción}

El objeto de este documento es plasmar los resultados obtenidos en el trabajo de
innovación de la asignatura de IA. Dicho trabajo se ha centrado en analizar uno
de los avances más recientes en este campo: la introducción de Ecuaciones en
Derivadas Ordinarias (EDOs) en el campo de las redes neuronales (Neural
Networks), concretamente en el campo de las ResNet (Residual Networks).

A lo largo de este documento explicaremos la idea y el funcionamiento de las ODENets y su impacto en el desarrollo de las redes neuronales, sobre todo en aquellas que procesan datos que no se obtienen en intervalos regulares. Además también analizaremos sus posibles aplicaciones e impacto en la sociedad.

\chapter{Desarrollo de la idea}

Como ya hemos mencionado, estudiaremos la introducción de EDOs en el campo de
las reades neuronales. Más concretamente en el campo de las ResNet (Residual
Networks).

A lo largo de esta sección (y de las posteriores) se asumirá un conocimiento
básico sobre el funcionamiento de las redes neuronales clásicas. Si el lector no
está familiarizado con los conceptos clásicos, como el concepto de ``neurona'',
``capa'', etc.. se recomienda \cite{brown17} como fuente que proporcionará el
conocimiento necesario para comprender este documento, aunque se recomienda
encarecidamente investigar más sobre el tema antes de proceder a la lectura de
este documento.

\section{ResNets}

Para poder comprender el origen de la idea y de su implementación, es necesario
primero un pequeño trasfondo en el campo de las Residual Networks, en las cuales
nos centraremos ahora.

Las Residual Networks nacen para resolver uno de los mayores problemas que
existen con las redes neurales: el problema de las capas. A la hora de crear un
modelo de red neural una de las decisiones que hay que tomar es el número de
capas de las que se compondrá nuestro modelo. A día de hoy no existe ningún
método para determinar el número de capas que debería tener una red para
funcionar correctamente. Ya que pocas capas provoca \textit{underfitting}, es
decir, que nuestra red no obtiene buenos resultados, pero muchas tener muchas
capas provoca \textit{overfitting}, es decir, que nuestra red es tan buena en
los casos de prueba que no es capaz de obtener buenos resultados en problemas
a los que no se ha enfrentado con anterioridad. A lo largo de los años han
aparecido muchos métodos para prevenir el \textit{overfitting}, y uno de ellos
son las ResNet (Residual Networks) que aunque no están exentas de este
problema, permiten mitigarlo bastante. La idea detrás de las ResNet es permitir
que sea la propia red la que ``decida'' cuantas capas tiene que tener.

Para ello, en cada capa además de tener como \texttt{input} el resultado de la
capa anterior, añadimos la entrada de la capa anterior.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \resizebox{\textwidth}{!}{
            \begin{tikzpicture}
                \node (A) at (0,0) {$h_n$};
                \node[draw=black] (B) at (2,0) {$f_n$}
                    edge[<-] (A);
                \node (D) at (4,0) {$h_{n+1}$}
                    edge[<-] (B);
            \end{tikzpicture}
        }
        \caption{Modelo de capa tradicional}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \resizebox{\textwidth}{!}{
            \begin{tikzpicture}
                \node (A) at (0,0) {$h_n$};
                \node[draw=black] (B) at (2,0) {$f_n$}
                    edge[<-] (A);
                \node[circle,draw=black] (C) at (4,0) {$+$}
                    edge[<-] (B)
                    edge[<-,bend right=40] (1,0);
                \node (D) at (6,0) {$h_{n+1}$}
                    edge[<-] (C); 
            \end{tikzpicture}
        }
        \caption{Modelo de capa de ResNet}
    \end{subfigure}
\end{figure}

Es decir, si $h_n$ es la entrada de la capa $n$, en una red neural ``al uso''
podemos modelar la salida ($h_{n+1}$) como
\[
    h_{n+1} = f_n\left( h_n \right)
\]
Sin embargo, en una ResNet, la ecuación previa queda de la forma
\[
    h_{n+1} = h_{n} + f_n\left( h_n \right)
\]
donde llamamos a $f_n$ la función residuo, de ahí el nombre Residual Network.

Es importante recalcar ahora cual es nuestro objetivo, nuestro objetivo es
encontrar una colección de $f_n$ de tal forma que $h_N$ sea el esperado (con $N$
el número de capas de la red). De forma que durante el proceso de aprendizaje
estaremos variando las $f_n$ para ajustarlas a nuestro objetivo.

La gran ventaja de las ResNet sobre las redes convencionales es que es posible
``eliminar'' una capa. Si ponemos $f_n = 0$ tenemos $h_{n+1} = h_{n} +
\cancel{f_n\left( h_n \right)}$ de tal forma que a efectos prácticos, la red se
ha ``saltado'' una capa. Es interesante notar como este mismo efecto también se
puede conseguir en una red neural tradicional poniendo $f_n(h) = h$ (es decir,
poniendo $f_n$ como la función identidad), sin embargo, durante el proceso de
aprendizaje es muy complicado que aparezca la función identidad, en cambio es
muy fácil que aparezca la función 0.

Esta facilidad para ``saltarse'' capas permite que podamos entrenar redes con
muchas más capas sin incurrir en \textit{overfitting} y por lo tanto crear
mejores modelos en menos tiempo.

Para nuestro análisis sobre las Neural Ordinary Differential Equations no
necesitamos profundizar más en este tema, aunque se trata de un area muy
interesante de investigación. De hecho existen más de 1000 artículos publicados
sobre este tema, aún con todo, queda mucho terreno por explorar en este campo
que podríamos decir que está en pañales. Más concretamentes, el primer artículo
\cite{micro15} data de 2015 y servirá como un punto de partida para el lector
interesado en el tema.


\section{Ecuaciones en Derivadas Ordinarias}

Para entender este avance, también necesitaremos estar algo familiarizados con
el concepto de Ecuaciones en Derivadas Ordinarias o EDOs para abreviar. Aquí tan
solo se hará un breve repaso y una explicación sencilla del tema, ya que se
trata de un campo muy amplio y con multitud de investigación realizado en él. Se
recomienda al lector poco familiarizado con este campo la profundización en
algunos aspectos, \cite{wode} es un buen lugar para empezar. Si el lector está
interesado en el tema y deseea tener una base matemática más consistente y
entendimiento más profundo de las EDOs, \cite{brown19} es el lugar donde
comenzar.

Una EDO (Ecuación en Derivadas Ordinarias) es una ecuación funcional
\cite{wfe} en la cual la función incógnita depende de sus propias derivadas. Un
ejemplo muy clarificador de este tipo de ecuaciones son las EDOs lineales:
\[
    0 = a_n f^{(n)}(t) + a_{n-1} f^{(n-1)}(t) + \cdots + a_2 f^{(2)}(t) +
    a_1 f^{(1)}(t) + b(t)
\]
donde $f \colon \mathbb{R}^m \to \mathbb{R}$ es nuestra fución incognita,
$f^{(i)}$ es la $i$-ésima derivada de $f$, $b(t)$ es una función conocida y
$a_i$ son constantes conocidas.

Un caso particular de una EDO lineal podria ser
\[
    f^\prime(t) + f(t) = 0
\]
Cuya solución es $f(t) = c e^{-t}$ con $c$ una constante cualquiera. Este
ejemplo nos da otra pista sobre las EDOs: precisamos de una condición inicial
para determinar de forma unívoca la respuesta a una ecuación. En el caso
anterior, si ponemos por ejemplo $f(0) = 2$ entonces $f$ queda determinada de
forma unica: $f(x) = 2 e^{-t}$.

A partir de ahora, tan solo nos centraremos en las EDOs de primer orden, es
decir, las ecuaciones en las cuales solo interviene la primera derivada de $f$.
Está simplificación se debe a dos motivos
\begin{itemize}
    \item Para la comprensión de las ODENets solo se precisan EDOs de primer
        orden.
    \item Las EDOs de ordenes superiores se pueden transformar en EDOs de primer
        orden aumentando $m$ y cambiando un poco la ecuación.
\end{itemize}

Un modelo más general de EDOs de primer orden es el siguiente
\[
    F(t, f, f^\prime) = 0
\]
Nosotros ``restringiremos'' un poco más nuestro espacio de EDOs admisibles y
solo trabajaremos con EDOs explicítas, es decir, las que se pueden expresar de
la forma
\[
    f^\prime(t) = F(t, f)
\]
donde $F$ es una función conocida. Como se describe en \cite{boyce86}, si
conocemos $f(t_0)$ entonces\footnote{Se necesita también que
$\frac{\partial F}{\partial f}$ sea continua sobre un rectángulo} existe una
única solución a la ecuación planteada.

El problema principal es que la mayoría de EDOs (incluso a las que nos hemos
restringido) no tienen solución analítica, es decir, no existe una expresión
exacta que podamos escribir. Por ello, la gran mayoría de la investigación se
basa en aproximar la solución de la EDO mediante métodos numéricos. Es
precisamente esta amplia investigación previa en métodos numéricos la que hace
tan interesante esta conexión con las redes neuronales.

\section{El Método de Euler}

El método de Euler es el método más primitivo para la resolución numérica de
EDOs explícitas y se basa en una aproximación de Taylor de primer orden.

Por Taylor, podemos aproximar $f$ como
\[
    f(t) = f(t_0) + f^\prime(t_0)(t - t_0) + \mathcal{O}\left( \left|t -
    t_0\right|^2 \right)
\]
Ahora tomamos $\Delta t$ pequeña y escribimos $t_1 = t_0 + \Delta t$ y tenemos
\[
    f(t_1) = f(t_0) + f^\prime(t_0) \Delta t + \mathcal{O}\left( \Delta t^2 \right)
\]
negligiendo ahora los errores, obtenemos
\[
    f(t_1) = f(t_0) + \Delta t f^\prime(t_0)
\]
Iterando ahora este proceso podemos obtener $f(t_2), f(t_3), \dots, f(t_N)$.
Uniendo ahora estos puntos, obtenemos una aproximación de $f$. Notemos que esta
ecuación se corresponde con la interpretación física que podríamos tener:
Aproximamos la función $f$ por su derivada, damos un paso pequeño en la derivada
y repetimos el proceso. Una visualización del proceso podría ser la siguiente:

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[domain=0:2,legend pos=outer north east]
            \addplot[blue,mark=none] {sin(deg(x))};
            \addplot[red,mark=*] coordinates { (0,0) (2/5,2/5) (4/5,0.768424)
            (6/5,1.0471) (8/5,1.192) (2,1.18)};
            \legend{Exacta, Euler}
        \end{axis}
    \end{tikzpicture}
    \caption{Comparación entre la solución exacta de una ODE y el método de
    Euler}
\end{figure}

A pesar de que el efecto está amplificado por la elección de una $\Delta t$
relativamente alta, se puede observar como la aproximación de la solución por el
método de Euler se aleja significativamente de la solución deseada.
Afortunadamente existen otros métodos mucho más precisos y eficientes como los
métodos Runge-Kutta \cite{wrk}.

\section{El paralelismo entre Euler y ResNet}

En está sección nos centraremos en el paralelismo existente entre dos tópicos
tan dispares como lo son las ResNets y el método de Euler. Para comenzar,
concretaremos como puede llevar la aproximación de una función por su derivada a
la solución de una EDO explícita.

Recordemos que en nuestra EDO teníamos $f^\prime(t) = F(t, f)$ y $f(t_0) = c_0$.
En el método de Euler aproximamos la función en ciertos puntos, concretamente,
aproximamos la función en los puntos $t_0, t_1, t_2, \dots, t_N$ donde $t_i =
t_{i-1} + \Delta t$. Podriamos llamar a estas aproximaciones $h_0, h_1, h_2, \dots, h_N$,
es decir, $h_i = \tilde{f}(t_i)$ donde $\tilde{f}$ es nuestra aproximación. Las
ecuaciones adaptadas a la nueva notación son:
\begin{equation}
    \label{eq:euler}
    h_{i+1} = h_i + \Delta t f^\prime(t_i) = h_i + \Delta t F(t_i, h_i)
\end{equation}

Recordemos ahora, cual era el método que empleabamos para calcular el resultado
de la ResNet:
\[
    h_{n+1} = h_n + f_n(h_n)
\]
Podemos ahora construir una función
\[
    F(h, t) = f_t(h)
\]
notemos que $F$ solo está definida para valores discretos de $t$ entre $0$ y
$N-1$. En particular, podemos escribir $t_0 = 0, t_1 = 1, \dots t_N = N$ y
obtendremos
\[
    h_{n+1} = h_n + F(h_n, t_n)
\]
Si además observamos que $\Delta t = t_{i+1} - t_i = 1$, podemos escribir la
ecuación anterior como
\begin{equation}
    \label{eq:resnet}
    h_{n+1} = h_n + \Delta t F(h_n, t_n)
\end{equation}

Ahora, podemos observar la similaridad entre \ref{eq:euler} y \ref{eq:resnet}.
Es decir, podríamos interpretar una ResNet como una aproximación por el método
de Euler de la función $f$. Por la tanto, uno puede hacerse la pregunta de si
podemos entonces aplicar un método mejor que Euler para aproximar dicha función.
La respuesta a dicha pregunta es afirmativa y es el tema que nos ocupa en la
siguiente sección.

\section{ODENet}

Una vez vista la similitud entre las ResNet y las EDOs, formularemos un modelo
de red neural basandonos en los \textit{ODE solvers} (solucionadores de EDOs de
forma numérica). Y los trataremos como si fuera una \textit{blackbox} (caja
negra), es decir, supondremos que existe una función ``mágica'' \texttt{ODEsolve}
que dependiendo de los parámetros, nos devuelve una aproximación de la EDO que
le planteamos.

Si consideramos la EDO $h(t) = F(t, h^\prime, \theta)$ con la condición inicial $h(t_0)
= h_0$ (con $\theta$ y $h_0$ parámetros conocidos), podemos asumir que existe una función
\[
    \texttt{$h_N$ = ODEsolve($h_0$, $F$, $t_0$, $t_N$, $\theta$)}
\]
Donde $h_N \approx h(t_N)$, es decir, existe una función que nos devuelve una
aproximación de la función solución en un punto dado.

Modificaremos el código en \texttt{python} para las ResNets para obtener nuestro
código para ODENets.
\begin{lstlisting}[language=Python]
def ResNet(h):
    for t in range(N):
        h = h + nnet(h, param[t])
    return h
\end{lstlisting}
Donde $\texttt{param} = \theta$ es el vector de matrices que contiene los parámetros de
la ResNet y $\texttt{N}$ es el número de capas de la red. Podemos reescribir este código
como
\begin{lstlisting}[language=Python]
def F(h, t, param):
    return nnet(h, param[t])

def ResNet(h):
    for t in range(N):
        h = h + F(h, t, param):
    return h
\end{lstlisting}
\pagebreak
Podemos ahora ``complicar'' un poco más el código escribiendo
\begin{lstlisting}[language=Python,float=h,floatplacement=H]
def F(h, t, param):
    return nnet(h, param[t])

def ResNet(h):
    return metodoEuler(h, F, 0, N, param, delta_t=1)
\end{lstlisting}

Ahora, el siguiente paso paso parece evidente: cambiar el método de Euler por un
método mejor para resoler EDOs.
\begin{lstlisting}[language=Python]
def F(h, t, param):
    return nnet([h, t], param)

def ODENet(h):
    return ODESolve(h, F, 0, t_N, param)
\end{lstlisting}
Obteniendo así una implementación de una ODENet.

\chapter{Innovación}\label{sec:inno}

La innovación de este diseño con respecto a los previos es la modelización del
problema como un sistema dinámico en lugar de aproximarlo de forma estática.
Esto supone un cambio de paradigma, pero no necesariamente una mejora en el
rendimiento. 

\section{Memoria}
La experimentación nos demuestra que en general las ODENets no suponen una
mejora considerable respecto a las ResNets con muchas capas. En realidad esto es
esperable si nos fijamos detalladamente en la interpretación que hemos hecho de
las ResNets y de las ODENets: a medida que aumentamos las capas de la ResNet,
reducimos el paso de Euler y por lo tanto mejoramos la aproximación. Es decir,
no obtenemos beneficio real en cuanto a mejoría de la solución. Sin embargo,
este método tiene una clara ventaja sobre las ResNet: que las ODENet tienen un
gasto de memoria constante respecto al ``número de capas'', mientras que las
ResNet tienen un gasto lineal.

\section{Número de capas}
Otra de las grandes ventajas que ofrece esta nueva perspectiva es que no se
necesita expecificar el número de capas, entre otras cosas porque el en las
ODENets no existe tal cosa como el número de capas, aunque para hacer
comparativas podemos tomar el número de evaluaciones de la función. Si empleamos
un método adaptativo el número de evaluaciones se ``calcula'' automaticamente,
de forma que no se necesita explicitar antes de empezar a entrenar el modelo.
El número de evaluaciones será incluso variable dependiendo de la entrada en
cuestión, es decir, que las ODENets usan un ``número de capas'' pequeño para
entradas ``fáciles'' y utiliza un ``número de capas'' grande cuando la entrada
es más ``dificil''.

\section{Adjoint method}
La última gran virtud de este nuevo paradigma es a la hora de entrenar el
modelo. Gracias a \cite{node} podemos emplear el \textit{Adjoint sensitivity
method} para calcular las derivadas de la red y actualizar los coeficientes, lo
que tradicionalmente se conoce
como \textit{backward pass}. Gracias a este método nos ahorramos calcular las
derivadas a través del \textit{ODE Solver} en cuestión (lo cual comporta gran
cantidad de errores numéricos así como gran trabajo computacional). Este método
por el contrario, nos permite calcular la derivada evaluando la función más o
menos la mitad de veces que en el pase normal (el \textit{fordward pass}). Si lo
pensamos en los términos tradicionales, sería como en una red neural de $M$
capas, calcular la derivada evaluando solo $\frac{M}{2}$ capas. Lo cual supone
una gran reducción de coste computacional.

\chapter{Impacto}
Es relativamente complicado medir el impacto de este avance debido a que se
trata de un avance muy reciente (sobre todo los avances en el uso del
\textit{Adjoint method} que han permitido la reducción del coste computacional).

\section{Impacto en la comunidad investigadora}
A pesar de lo reciente del avance, la reacción por parte de la comunidad
investigadora y educativa no se ha hecho esperar, son muchos los papeles que se
han publicado ya en torno a las Neural ODE (NODE). Para concretar, el articulo
\cite{node} ya contiene 44 citaciones, entre ellas, cabe destacar \cite{anode}
donde se plantean algunas de las limitaciones de las ODENets y se plantean
soluciones a las mismas.

Es muy destacable el reconocimiento obtenido en
\href{https://nips.cc/}{NeurIPS}, la conferencia sobre IA más grande del mundo,
con 4854 papeles mandados. Solo cuatro de ellos recivieron el premio al mejor
papel, \cite{node} fue uno de ellos. Se evidencia así el interés de la comunidad
en este campo tan reciente.

\section{Primeras aplicaciones}
La primera implementación en emplear el método descrito en \cite{node} se
encuentra en GitHub, concretamente en \cite{torchdiffeq}, allí se encuentra el
código necesario para conducir algunos experimentos independientes. Sin embargo,
en el artículo ya se exploran algunas aplicaciones, en concreto se menciona
el campo de \textit{Supervised learning}, \textit{Continuous Normalizing Flows}
y \textit{time-series model}. Este último es particularmente interesante, ya que
se trata de predecir un modelo general a partir de muestras tomadas en
intervalos de tiempo irregulares. Este es el caso general cuando se trata de
problemas médicos: muestras muy caoticas y dispersas de las que deducir un
modelo general. En todos los casos, las ODENets han demostrado una prometedora
eficiencia.

\section{Impacto en la Universidad de Toronto}
Es el momento de analizar el impacto de este avance allí donde se realizó. Nos
centraremos en \cite{node} ya que se trata del papel más importante hasta la
fecha. 

Centremonos primero en los riesgos asumidos por la universidad. El único riesgo
que se nos podría venir a la cabeza es el riesgo económico, sin embargo, no
consideramos que en una entidad pública (como lo es la universidad) se deban
considerar riesgos económicos ya que, muy probablemente, el resultado del proceso
en el cual se invierte no tenga consecuencias económicas. En otras palabras, no
tiene sentido plantearse la ``pérdida de dinero'' cuando el resultado del
proceso no tiene un impacto económico directo. Aún así, desde un tiempo a esta
parte se ha venido tratando de medir la exito de una investigación academica con
diferentes métodos como por ejemplo el ``factor de impacto''.

Sin embargo, como se demuestra en \cite{stapel} este tipo de medidas tan solo
lleva a una creación masiva de articulos basura y a una pérdida general de
calidad en la producción de documentos científicos. Este hecho es ampliamente
respaldado a nivel mundial con entidades tan importantes como \cite{dora}.
Citando a \cite{manifesto}
\begin{quotation}
Instead of valuing a contribution to public debate, these articles are now
regarded as advertisements for the own university: their ‘value’ is calculated
from the advertising rates for that part of the article in which this university
is mentioned.
\end{quotation}

Es decir, que los artículos y su impacto se valoran únicamente con respecto a la
cantidad de publicidad que dan a la universidad y no por su contribución a la
ciencia ni por su calidad en contenidos.

También resulta delicado medir los beneficios de la investigación ya que, como
se ha mencionado, no tiene un impacto económico directo sino un impacto social.
De hecho los beneficios de la investigación son incalculables: ``¿qué beneficios
tiene el teorema de Pitágoras?'' ``¿Qué beneficios tiene la relatividad
general?'' ``¿Qué beneficios tiene un nuevo algoritmo de IA?''. Los beneficios
de la investigación \textit{per se} ya están descritos en \ref{sec:inno} y
nosotros planteamos que las investigaciones no deben tener necesariamente una compensación
económica.

Bajo nuestra humilde opinión, a la hora de determinar la ``viabilidad''
de una investigación se debería de considerar los riesgos como 0 y los
beneficios como infinito. Es decir, bajo nuestro punto de vista, no existen
investigaciones ``inviables'', sino simplemente investigaciones.

Nuestra propuesta se alinea más con la que se defiende en \cite{stallman} y en
la \href{https://www.fsf.org/}{FSF} en su campaña
\href{http://endsoftpatents.org/}{End of Software Patents}, en la cual no exite tal cosa como la
``propiedad intelectual''. Por lo tanto, no se debería recompensar la
investigación con monopolios temporales gracias a las patentes. Como demuestra
\cite{patents} un modelo de investigación en el cual se introducen las patentes
lleva a un nivel de desarrollo de la sociedad mucho menor que en el modelo en el
cual no se introducen.

Si nos restringimos al área del software, el hecho es todavía más grave. Commo
se ha dictado sentencia en USA con las palabras
\begin{quote}
    math is unpatentable because it is a "law of nature", that is to say
    a "scientific truth", and as such it can never be "invented", only
    "discovered", and patents are not granted for discoveries.
\end{quote}
y por otro lado, la EPO (Oficina Europea de Patentes) propuso
\begin{quote}
    computer programs were to be understood as a 'mathematical
    application of a logical series of steps in a process which was no different
    from a mathematical method
\end{quote}
concluyendo así que el software es impatentable.

Existen otras muchas razones para la abolición de las patentes de sofware, como
por ejemplo que
\href{http://en.swpat.org/wiki/Software_patents_harm_SMEs}{perjudican a las
PIMEs}, \href{http://en.swpat.org/wiki/Harms_to_education}{dañan la educación},
\href{http://en.swpat.org/wiki/Reducing_innovation_and_research}{reducen la
innovación y investigación} o atentan contra la
\href{http://en.swpat.org/wiki/Freedom_of_expression}{libertad de expresión}
entre otros muchos efectos negativos\footnote{Para más información, referirse a
la \href{http://en.swpat.org/wiki/Why_abolish_software_patents}{wiki de la ESP}}.

Así, concluimos que no podemos evaluar el impacto en el mercado de esta investigación, pues ha sido conducida en la Universidad de Toronto, que una universidad públca, y el objetivo de la universidad pública no es ganar dinero, sino tener un impacto social positivo.

Además, consideramos que no debería ser la función de la universidad preguntar a sus estudiantes sobre los posibles beneficios y riesgos económicos que una investigación tecnológica o científica pueda tener para una empresa, dado que esta investigación, sea o no rentable económicamente, a largo plazo será beneficiosa para la sociedad en su conjunto.
% TODO
% no control of time-cost of the model

\nocite{*}
\printbibliography[title={Bibliografía},heading=bibintoc]

\end{document}
